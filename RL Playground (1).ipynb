{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article this is derived from : http://amunategui.github.io/reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a simple Q Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of points that are mapped to each other. \n",
    "# Our goal is to get from any point to the goal point, in this case 5\n",
    "\n",
    "points_list = [(0,4),(4,3),(2,3),(1,3),(4,5),(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rewards Matrix \n",
    "\n",
    "# We have 6 possible states and 6 possible actions\n",
    "# We have 6 actions because, lets say from 3, we can choose an action to go to state 2, 1 or 4. \n",
    "# Lets look at it another way, we have the following possible actions\n",
    "# Up, Down, Front, back, slant in, slant out ( Ok bad choice of names, but I am creatively impaired, sooo ...)\n",
    "MATRIX_SIZE = 6\n",
    "goal = 5\n",
    "\n",
    "R = np.matrix(np.ones(shape=[MATRIX_SIZE,MATRIX_SIZE]))\n",
    "R = R * -1 # We assign -1 to every value\n",
    "\n",
    "for points in points_list:\n",
    "    R[points] = 0\n",
    "    if points[1] == goal:\n",
    "        R[points] = 100\n",
    "    points = points[::-1]\n",
    "    R[points] = 0\n",
    "\n",
    "R[goal,goal] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q matrix : Our brain\n",
    "# Initially, we will have an empty Q matrix, which as we iterate through the different possible states and actions\n",
    "# the RL process will populate with values it can use to make decisions. \n",
    "\n",
    "Q = np.matrix(np.zeros(shape=[MATRIX_SIZE,MATRIX_SIZE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at Rewards and Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rewards Matrix is \n",
      " \n",
      " [[ -1.  -1.  -1.  -1.   0.  -1.]\n",
      " [ -1.  -1.  -1.   0.  -1. 100.]\n",
      " [ -1.  -1.  -1.   0.  -1.  -1.]\n",
      " [ -1.   0.   0.  -1.   0.  -1.]\n",
      " [  0.  -1.  -1.   0.  -1. 100.]\n",
      " [ -1.   0.  -1.  -1.   0. 100.]]\n",
      "\n",
      " \n",
      "The Q table Matrix is \n",
      " \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The rewards Matrix is \\n \\n {}\".format(R))\n",
    "print(\"\\n \")\n",
    "print(\"The Q table Matrix is \\n \\n {}\".format(Q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://mnemstudio.org/ai/path/images/map3a.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://mnemstudio.org/ai/path/images/map3a.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Please refer to the image above for better clarity \n",
      "\n",
      "2) As we can see, the available actions from state 1 is [3 5] \n",
      "\n",
      "3) The action we have chosen to take is to go to 3, from state 1 \n",
      "\n",
      "4) Here we have pause a little and introduce a hyper parameter called Gamma\n",
      "   Gamma is a paramter that is used to control if the model needs to focus on immediate reward or future rewards \n",
      "\n",
      "5) The Equation for our Q learner is as follows\n",
      "   Q[state,action] = R[state,action] + gamma * Max(Q[next state,all_actions])\n"
     ]
    }
   ],
   "source": [
    "# Now let us create the structure of the simple Q Learner\n",
    "print(\"1) Please refer to the image above for better clarity \\n\")\n",
    "\n",
    "# Current State\n",
    "current_state = 1\n",
    "\n",
    "# Let us get all possible actions for current state\n",
    "# From the rewards function, we can see which direction we can go in \n",
    "\n",
    "def available_actions(state):\n",
    "    rewards_row_state = R[state,]\n",
    "    av_actions = np.where(rewards_row_state >= 0)[1] # In our case, any value ge 0 is a possible action we can take from our current state\n",
    "    return av_actions\n",
    "    \n",
    "actions = available_actions(current_state)\n",
    "print(\"2) As we can see, the available actions from state {} is {} \\n\".format(current_state,actions))\n",
    "\n",
    "# Lets say, we choose one action from the available actions, so we choose randomly\n",
    "\n",
    "def choose_random_action(actions):\n",
    "    action = random.choice(actions)\n",
    "    return action\n",
    "\n",
    "action = choose_random_action(actions)\n",
    "print(\"3) The action we have chosen to take is to go to {}, from state {} \\n\".format(action,current_state))\n",
    "\n",
    "# we now have information about our current state, current action we are taking, \n",
    "# and possible next states and actions we can take\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "print(\"4) Here we have pause a little and introduce a hyper parameter called Gamma\")\n",
    "print(\"   Gamma is a paramter that is used to control if the model needs to focus on immediate reward or future rewards \\n\")\n",
    "\n",
    "# Now we develop our Q update function\n",
    "\n",
    "print(\"5) The Equation for our Q learner is as follows\")\n",
    "print(\"   Q[state,action] = R[state,action] + gamma * Max(Q[next state,all_actions])\")\n",
    "\n",
    "def Q_update(state,action,gamma):\n",
    "    # First the max Q part\n",
    "    Max_Q = np.max(Q[action,])\n",
    "    Q[state,action] = R[state,action] + gamma * Max_Q\n",
    "    return Q\n",
    "\n",
    "Q = Q_update(current_state,action,gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create a Monte Carlo simulation of the Markov Decision Process to converge this Q matrix (Our Brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    current_state = random.choice([0,1,2,3,4,5]) # Step 1\n",
    "    actions = available_actions(current_state) # Step 2\n",
    "    action = choose_random_action(actions) # Step 3\n",
    "    Q = Q_update(current_state,action,gamma) # Step 4\n",
    "    \n",
    "# We can normalize Q Matrix values for a better idea of what is going on\n",
    "\n",
    "Q = Q/np.max(Q)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at the Image again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://mnemstudio.org/ai/path/images/map3a.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://mnemstudio.org/ai/path/images/map3a.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most efficient path from state 4 is [4, 5]\n"
     ]
    }
   ],
   "source": [
    "current_state = 4\n",
    "step = [current_state]\n",
    "\n",
    "while current_state != goal:\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "    step.append(next_step_index[0])\n",
    "    \n",
    "    if len(next_step_index) > 1:\n",
    "        next_step_index = random.choice(next_step_index)\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    current_state = next_step_index\n",
    "        \n",
    "print(\"The most efficient path from state {} is {}\".format(step[0],step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
