{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article this is derived from : http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a simple Q Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of points that are mapped to each other. \n",
    "# Our goal is to get from any point to the goal point, in this case 5\n",
    "\n",
    "environment = [(0,4),(4,3),(2,3),(1,3),(4,5),(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rewards Matrix \n",
    "\n",
    "# We have 6 possible states and 6 possible actions\n",
    "# We have 6 actions because, lets say from 3, we can choose an action to go to state 2, 1 or 4. \n",
    "# Lets look at it another way, we have the following possible actions\n",
    "# Up, Down, Front, back, slant in, slant out ( Ok bad choice of names, but I am creatively impaired, sooo ...)\n",
    "states = 6\n",
    "actions = 6\n",
    "goal = 5\n",
    "\n",
    "R = np.matrix(np.ones(shape=[states,actions]))\n",
    "R = R * -1 # We assign -1 to every value\n",
    "\n",
    "for row in environment:\n",
    "    R[row] = 0\n",
    "    if row[1] == goal:\n",
    "        R[row] = 100\n",
    "    row = row[::-1]\n",
    "    R[row] = 0\n",
    "\n",
    "R[goal,goal] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q matrix : Our brain\n",
    "# Initially, we will have an empty Q matrix, which as we iterate through the different possible states and actions\n",
    "# the RL process will populate with values it can use to make decisions. \n",
    "\n",
    "Q = np.matrix(np.zeros(shape=[MATRIX_SIZE,MATRIX_SIZE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at Rewards and Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rewards Matrix is \n",
      " \n",
      " [[ -1.  -1.  -1.  -1.   0.  -1.]\n",
      " [ -1.  -1.  -1.   0.  -1. 100.]\n",
      " [ -1.  -1.  -1.   0.  -1.  -1.]\n",
      " [ -1.   0.   0.  -1.   0.  -1.]\n",
      " [  0.  -1.  -1.   0.  -1. 100.]\n",
      " [ -1.   0.  -1.  -1.   0. 100.]]\n",
      "\n",
      " \n",
      "The Q table Matrix is \n",
      " \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The rewards Matrix is \\n \\n {}\".format(R))\n",
    "print(\"\\n \")\n",
    "print(\"The Q table Matrix is \\n \\n {}\".format(Q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://mnemstudio.org/ai/path/images/map3a.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://mnemstudio.org/ai/path/images/map3a.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Please refer to the image above for better clarity \n",
      "\n",
      "2) As we can see, the available actions from state 1 is [3 5] \n",
      "\n",
      "3) The action we have chosen to take is to go to 3, from state 1 \n",
      "\n",
      "4) Here we have pause a little and introduce a hyper parameter called Gamma\n",
      "   Gamma is a paramter that is used to control if the model needs to focus on immediate reward or future rewards \n",
      "\n",
      "5) The Equation for our Q learner is as follows\n",
      "   Q[state,action] = R[state,action] + gamma * Max(Q[next state,all_actions])\n"
     ]
    }
   ],
   "source": [
    "# Now let us create the structure of the simple Q Learner\n",
    "print(\"1) Please refer to the image above for better clarity \\n\")\n",
    "\n",
    "# Current State\n",
    "currentstate = 1\n",
    "\n",
    "# Let us get all possible actions for current state\n",
    "# From the rewards function, we can see which direction we can go in \n",
    "\n",
    "def available_actions(state):\n",
    "    rewards_row_state = R[state,]\n",
    "    av_actions = np.where(rewards_row_state >= 0)[1] # In our case, any value ge 0 is a possible action we can take from our current state\n",
    "    return av_actions\n",
    "    \n",
    "actions = available_actions(currentstate)\n",
    "print(\"2) As we can see, the available actions from state {} is {} \\n\".format(current_state,actions))\n",
    "\n",
    "# Lets say, we choose one action from the available actions, so we choose randomly\n",
    "\n",
    "def choose_random_action(actions):\n",
    "    action = random.choice(actions)\n",
    "    return action\n",
    "\n",
    "action = choose_random_action(actions)\n",
    "print(\"3) The action we have chosen to take is to go to {}, from state {} \\n\".format(action,current_state))\n",
    "\n",
    "# we now have information about our current state, current action we are taking, \n",
    "# and possible next states and actions we can take\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "print(\"4) Here we have pause a little and introduce a hyper parameter called Gamma\")\n",
    "print(\"   Gamma is a paramter that is used to control if the model needs to focus on immediate reward or future rewards \\n\")\n",
    "\n",
    "# Now we develop our Q update function\n",
    "\n",
    "print(\"5) The Equation for our Q learner is as follows\")\n",
    "print(\"   Q[state,action] = R[state,action] + gamma * Max(Q[next state,all_actions])\")\n",
    "\n",
    "def Q_update(state,action,gamma):\n",
    "    # First the max Q part\n",
    "    Max_Q = np.max(Q[action,])\n",
    "    Q[state,action] = R[state,action] + gamma * Max_Q\n",
    "    return Q\n",
    "\n",
    "Q = Q_update(currentstate,action,gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create a Monte Carlo simulation of the Markov Decision Process to converge this Q matrix (Our Brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    current_state = random.choice([0,1,2,3,4,5]) # Step 1\n",
    "    actions = available_actions(current_state) # Step 2\n",
    "    action = choose_random_action(actions) # Step 3\n",
    "    Q = Q_update(current_state,action,gamma) # Step 4\n",
    "    \n",
    "# We can normalize Q Matrix values for a better idea of what is going on\n",
    "\n",
    "Q = Q/np.max(Q)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          76.54487665,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,  61.23590132,\n",
       "           0.        , 100.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,  61.23590132,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  75.08183323,  48.98872106,   0.        ,\n",
       "          76.54487665,   0.        ],\n",
       "        [ 61.23590132,   0.        ,   0.        ,  61.23590132,\n",
       "           0.        ,  95.68109582],\n",
       "        [  0.        ,  75.08183323,   0.        ,   0.        ,\n",
       "          76.54487665,  99.25092177]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at the Image again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://mnemstudio.org/ai/path/images/map3a.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://mnemstudio.org/ai/path/images/map3a.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most efficient path from state 4 is [4, 5]\n"
     ]
    }
   ],
   "source": [
    "currentstate = 4\n",
    "tracking = []\n",
    "tracking.append(currentstate)\n",
    "\n",
    "while currentstate != goal:\n",
    "    nextstep = np.where(Q[currentstate,] == np.max(Q[currentstate,]))[1]\n",
    "    tracking.append(nextstep[0])\n",
    "    \n",
    "    if len(nextstep) > 1:\n",
    "        nextstep = random.choice(nextstep)\n",
    "    else:\n",
    "        nextstep = nextstep\n",
    "    \n",
    "    currentstate = nextstep\n",
    "        \n",
    "print(\"The most efficient path from state {} is {}\".format(step[0],step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
